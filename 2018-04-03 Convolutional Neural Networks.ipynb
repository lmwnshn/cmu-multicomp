{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "06b9e325-df2f-4e9a-8a0f-5b0456c8c6c2"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "\n",
    "> Hi guys, the course I mentioned before is cs231n, which covers both neural network and convolutional neural network  \n",
    "> So we will start with the basics about neural network, loss function and optimizer for next Tuesday  \n",
    "> And we will be using pytorch as the main framework  \n",
    "> Ying 3/27\n",
    "\n",
    "Source material: http://cs231n.stanford.edu/syllabus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Nearest-Neighbor\n",
    "\n",
    "Memorize all the labeled data, compute the L1 distance $d(I_1, I_2) = \\sum_{p} \\left| I_1^p - I_2^p \\right|$, output the label with the smallest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# code adapted from lecture\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NearestNeighbor:\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        X is N x D examples\n",
    "        Y is 1 x N labels\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X is N x D data\n",
    "        \"\"\"\n",
    "        num_examples = X.shape[0]\n",
    "        \n",
    "        diffs = self.X_train[:,None] - X[None,:]\n",
    "        distances = np.sum(np.abs(diffs), axis=2)\n",
    "        labels = np.argmin(distances,axis=1)\n",
    "        predictions = self.y_train[labels]\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cat', 'dog'], dtype='<U3')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NearestNeighbor()\n",
    "train_data = np.array([[1,2], [2,1]])\n",
    "train_label = np.array(['cat', 'dog'])\n",
    "nn.train(train_data, train_label)\n",
    "nn.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$O(1)$ train, $O(n)$ predict. This is bad. We can live with slow training, we don't want slow predicting.\n",
    "\n",
    "Relatedly, we have\n",
    "\n",
    "# k-nearest neighbors\n",
    "\n",
    "Take the majority vote from $k$ nearest points (L2 distance)\n",
    "\n",
    "In practice, never used for image recognition:\n",
    "\n",
    "- slow\n",
    "- distance on pixels uninformative\n",
    "    - easy to cook up different images with same distance\n",
    "- curse of dimensionality\n",
    "- good teaching example though\n",
    "    - you now have a hyperparameter, k\n",
    "    - we can talk about train/valid/test sets\n",
    "    - cross-validation: in n-fold, we split the data into n equal folds, use n-1 for training and 1 for validation, average performance over different folds\n",
    "        - not actually used very frequently in deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Linear Classifier\n",
    "\n",
    "[Source](http://cs231n.github.io/linear-classify/)\n",
    "\n",
    "Lead-up to neural networks\n",
    "\n",
    "The problem with all the nearest-neighbor approaches is that they need you to store and compare against **all** the data. This sucks.\n",
    "\n",
    "What if instead, we used all the data to form a \"template\" to compare against? Then we can discard all the data and just keep the templates. Much faster.\n",
    "\n",
    "Idea: template is a linear function $f: R^D \\to R^k, f(x_i, W, b) = W x_i + b$, where $W$ represents weights and $b$ represents bias.\n",
    "\n",
    "Some notes:\n",
    "\n",
    "- Bias dimension: we can simplify this further by extending $x_i$ with a bias dimension of all 1's, then we just tack the bias onto the weight matrix.\n",
    "- Normalization: we usually subtract the mean from everything in our input to rescale\n",
    "- Limitation: because we're creating a template from our data, if you have horses facing left and horses facing right, the template becomes a 2 headed horse. Similarly, unable to deal with differences in color well.\n",
    "\n",
    "So how do we pick the right $W$ and $b$? Actually, how do we define **right**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prelude\n",
    "\n",
    "In a nearest-neighbor approach, for every input we had to (1) memorize all the data and (2) compare against all the data. This is space and time inefficient. Furthermore, the model was fast to train but slow to use: we would prefer the opposite of slow to train and fast to use.\n",
    "\n",
    "**Idea: what if we used all our data to form a \"template\"?**\n",
    "\n",
    "Recall that we have linear classifiers of the form $f(x_i, W, b) = W x_i + b$, equivalently $f(x_i, W) = W x_i$, where $x_i$ is an input image and $W$ is our weights matrix.\n",
    "\n",
    "Our weights matrix essentially IS our template. What does this mean?\n",
    "\n",
    "- Normalization: we should normalize our input by subtracting the mean image from everything (reasons come later)\n",
    "- Limitation: for a single linear classifier, if you're trying to recognize horses facing left and horses facing right, your template ends up being a 2-headed horse\n",
    "\n",
    "But rolling with linear classifiers for now: how do we know when we have the right weights matrix? How do we define **right**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "An objective way of measuring how wrong something is.\n",
    "\n",
    "For every item in our training set, our linear function will predict some score, and we know what the actual item should be.\n",
    "\n",
    "The loss function is a way to penalize the difference between the two, i.e. the further off your prediction, the higher your loss.\n",
    "\n",
    "We will go over two common loss functions:\n",
    "\n",
    "1. Multiclass Support Vector Machine\n",
    "2. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiclass Support Vector Machine Loss\n",
    "\n",
    "We define loss of the $i$th prediction as $L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\delta)$\n",
    "\n",
    "The score of the right class has to be greater than the score of the incorrect classes by at least $\\delta$, otherwise we linearly add the loss.\n",
    "\n",
    "Intuition: we don't want wrong predictions to come closer than $\\delta$ to our right class\n",
    "\n",
    "We might also see other types of penalty functions, e.g.\n",
    "\n",
    "- **Hinge loss:** $\\max(0, -)$\n",
    "- **Squared Hinge Loss (L2-SVM)**: $\\max(0, -)^2$\n",
    "\n",
    "\n",
    "Recall: we wanted a loss function so that we can find out the \"ideal\" weight\n",
    "\n",
    "Problem: the weight function is not unique! Suppose there exists a perfect $W$ with 0 loss. Then $2W$, $3W$, ... all have 0 loss too.\n",
    "\n",
    "Solution: regularization\n",
    "\n",
    "### Regularization\n",
    "\n",
    "We regularize by preferring one set of weights over the other. The most common regularization approach is to add the L2 norm,\n",
    "\n",
    "$R(W) = \\sum_k \\sum_l W_{k,l}^2$\n",
    "\n",
    "The bigger your weights, the worse your score.\n",
    "\n",
    "Hence our full multiclass SVM loss becomes\n",
    "\n",
    "$L = \\text{data loss} + \\text{regularization loss}$  \n",
    "$L = \\frac{1}{N} \\sum_i L_i + \\lambda R(W)$\n",
    "\n",
    "Question: why penalize larger weights instead of smaller ones?\n",
    "\n",
    "Answer: less overfitting, reduce the influence of a single input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Example loss function\n",
    "\n",
    "def L(X, y, W):\n",
    "    \"\"\"\n",
    "    X: all training examples, N_features x N_examples\n",
    "    y: array of all correct class indexes, N_examples long\n",
    "    W: weights, N_classes x N_features\n",
    "    \n",
    "    DOESN'T INCLUDE REGULARIZATION\n",
    "    \n",
    "    returns: loss for each example\n",
    "    \"\"\"\n",
    "    delta = 1.0 # hyperparameter - in practice can always be set to 1, since it counterbalances weights\n",
    "    scores = W.dot(X) # N_classes x N_examples\n",
    "    yi_scores = scores[np.arange(scores.shape[0]), y] # inside every range element, pick yth thing\n",
    "    margins = np.maximum(0, scores - np.matrix(yi_scores).T + delta)\n",
    "    margins[np.arange(X.shape[1]), y] = 0 # don't include in loss calculation\n",
    "    loss = np.mean(np.sum(margins, axis=0))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3333333333333335\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,2,3], [1,2,3]]) # 3 examples, 2 features\n",
    "y = np.array([0,1,2]) # 3 known labels\n",
    "W = np.array([[1,0],[0,1],[1,1]]) # 3 classes by 2 features\n",
    "print(L(X,y,W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Other SVMs we can look at are\n",
    "\n",
    "- binary: equivalent where classes=2\n",
    "- OVA (one versus all): train a binary SVM per class\n",
    "- AVA (all versus all): pairwise?\n",
    "- structured: max margin(correct class, highest-scoring incorrect runner-up class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax classifier\n",
    "\n",
    "Generalized analogue of a logistic regression to multiple classes.\n",
    "\n",
    "Compare:\n",
    "\n",
    "- Multiclass SVM outputs raw class scores that don't really have a meaning\n",
    "- Softmax will output normalized class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In softmax, we keep the same weight function, but replace hinge loss with cross-entropy loss:\n",
    "\n",
    "$L = -\\log \\left( \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} \\right) = -log (s(v))$\n",
    "\n",
    "where $s(v)$ is the softmax function that takes an arbitrary vector of real valued scores and squashes it into a vector of values between 0 and 1 that sums to 1.\n",
    "\n",
    "We interpret the INPUT scores as unnormalized log probabilities for each class, and OUTPUT as a normalized class probability.\n",
    "\n",
    "We can draw further comparisons to MLE or MAP, but that's probably beyond the scope of today.\n",
    "\n",
    "Technicalities:\n",
    "\n",
    "- numerical stability is a concern with the exponent. We can compute $\\frac{C e^{f_{y_i}}}{C \\sum_j e^{f_j}}$ instead, where $\\log C = - \\max_j f_j$.\n",
    "- SVM uses hinge loss / max-margin loss\n",
    "- softmax is technically cross-entropy loss, but people call it softmax because of the squashing up there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization\n",
    "\n",
    "Okay, so now that we have a loss function, we know how well our linear models work. Now how do we start?\n",
    "\n",
    "1. Random search: just try random weights. Kind of stupid, but it gives us a good starting point.\n",
    "2. Random search in nearby direction: a little better\n",
    "3. Gradient descent: find the direction that minimizes our loss, and head that way\n",
    "\n",
    "Gradient descent sounds promising. We can either do it numerically or analytically with calculus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    fx = f(x)\n",
    "    grad = np.zeros(x.shape)\n",
    "    step = 10e-6\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        x_0 = x[ix]\n",
    "        x[ix] += step\n",
    "        fx_new = f(x)\n",
    "        x[ix] = x_0\n",
    "        grad[ix] = (fx_new - fx) / step\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33333333,  0.66666667],\n",
       "       [-0.66666667, -0.33333333],\n",
       "       [ 0.33333333,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2,1], [1,2,2]]) # 3 examples, 2 features\n",
    "y = np.array([0,1,2]) # 3 known labels\n",
    "W_0 = np.array([[0.5,0],[0,0.5],[2,3]]) # 3 classes by 2 features\n",
    "\n",
    "numerical_gradient(lambda W: L(X, y, W), W_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problem: REALLY slow. Needs to iterate over every single feature per update.\n",
    "\n",
    "Instead, we can compute it analytically by taking the derivative with respect to each $w_j$.\n",
    "\n",
    "For example, our earlier Multiclass SVM had the following loss function:  \n",
    "$L_i = \\sum_{j \\neq y_i} \\max \\left( 0, w_j^T x_i - w_{y_i}^T x_i + \\Delta \\right)$\n",
    "\n",
    "Taking the derivative, we have:  \n",
    "$\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j \\neq y_i} \\mathbb{1} \\left( w_j^T x_i - w_{y_i}^T x_i + \\Delta > 0 \\right) \\right) x_i$  \n",
    "$\\nabla_{w_{j}} L_i = \\mathbb{1} \\left( w_j^T x_i - w_{y_i}^T x_i + \\Delta > 0 \\right) x_i$\n",
    "\n",
    "i.e. we scale our input vector by the number of nonzero classes. That wasn't so bad.\n",
    "\n",
    "Notes:\n",
    "\n",
    "- a further optimization is Mini-Batch Gradient Descent, where you evaluate gradients over a smaller sample of your data (e.g. $256$ examples). \n",
    "    - if $n=1$, this is Stochastic Gradient Descent - usually not done because vectorized code is faster\n",
    "- notice that after coming up with the gradient, we need to pick a step size to travel by\n",
    "    - another hyperparameter, more of an art than a science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Instead of just having one linear function, let's have a whole bunch of functions and connect them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Backpropagation\n",
    "\n",
    "A way to compute gradients in small manageable parts via recursively applying the chain rule.\n",
    "\n",
    "Consider a single variable function.\n",
    "\n",
    "Instead of analytically coming up with the entire gradient - break everything into a computation graph.\n",
    "\n",
    "The units for the nodes are your choice. But for example, plus, mul, max gates.\n",
    "\n",
    "Then for each node, you know how the gradient works, so you just apply the chain rule.\n",
    "\n",
    "There's a few neat ways to think of it:\n",
    "\n",
    "- add is a distributor\n",
    "- max is a router\n",
    "- mul is a switcher\n",
    "\n",
    "Essentially just math though.\n",
    "\n",
    "Then for a multivar function, you'd just have lots of Jacobians flying around. But you really only care about the diagonal.\n",
    "\n",
    "Implementation notes:\n",
    "\n",
    "- each node should support a forward() / backward() API\n",
    "    - forward(): compute, store and output the gradient\n",
    "    - backward(): apply chain rule to get gradient of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most useful insights from neurons:\n",
    "\n",
    "- activation function / firing\n",
    "- stacking them in layers\n",
    "- degree of connectedness\n",
    "\n",
    "So for example, previously we had $f = Wx$\n",
    "\n",
    "Now we can stack another layer of that, $f = W_2 \\max(0, W_1 x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "There was a problem with our neural network formulation earlier.\n",
    "\n",
    "If we view one pixel = one feature, then for a small 32x32x3 image, you need 3072 weights!\n",
    "\n",
    "Simplifying assumption: input are images. Therefore, we only care about\n",
    "\n",
    "- width: how wide the image is\n",
    "- height: how tall the image is\n",
    "- depth: how many channels the image has (e.g. RGB has 3, CMYK has 4)\n",
    "\n",
    "(Note on notation: confusingly, sometimes depth of a neural network = number of layers)\n",
    "\n",
    "We therefore can transform images in a coherent and managed way between layers, without having to fully connect everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Layers\n",
    "\n",
    "There are three main types of layers in building a CNN:\n",
    "\n",
    "- Convolutional Layer\n",
    "- Pooling Layer\n",
    "- Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Convolutional Layer\n",
    "\n",
    "Parameters: set of learnable filters\n",
    "\n",
    "Each filter is spatially small across width/height but spans full depth\n",
    "\n",
    "We slide the filter across the image, producing a 2D activation map.\n",
    "\n",
    "The activation maps are stacked along the depth dimension, producing our output volume.\n",
    "\n",
    "\n",
    "Hyperparameters:\n",
    "\n",
    "- $F$, filter size: \"receptive field of neuron\", local connectivity\n",
    "- depth: number of filters that are looking at the same region of the input\n",
    "- $S$, stride: how smoothly we slide the filters over the image, wider stride => smaller output volume\n",
    "- $P$, zero-padding: how many zeros we pad around as a border, we can use it to control size of output volume\n",
    "\n",
    "Input volume size is usually $W$, formula for output size is $\\frac{Wâˆ’F+2P}{S}+1$.\n",
    "\n",
    "e.g. a 7x7 input, a 3x3 filter, stride 1, pad 0 => $\\frac{7-3+0}{1}+1 = 5$, we get a 5x5 output\n",
    "\n",
    "If we wanted an output that was the same size as the input, we could have set pad 1.\n",
    "\n",
    "**Parameter sharing**: simplifying assumption, if a feature is useful at (x1,y1), it should also be useful at (x2,y2). So you only have *depth* sets of parameters.\n",
    "- sometimes not applicable, e.g. eye vs nose recognition, so we relax it sometimes and have Locally Connected Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "\n",
    "CONV layers and fully connected layers can be converted between each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pooling Layer\n",
    "\n",
    "A pooling layer basically downsizes the spatial dimensions that we're working with. This is usually done with 3x3 filter with a stride of 2 or a 2x2 filter with a stride of 2.\n",
    "\n",
    "Some people don't believe in pooling, instead preferring more CONV layers. To reduce dimensions, they suggest using larger strides once in a while. In particular,\n",
    "\n",
    "> Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalization Layer\n",
    "\n",
    "In practice, normalization layers don't seem to be very useful / minimal contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architecture\n",
    "\n",
    "\n",
    "One of the most common architectures is \n",
    "\n",
    "`INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC`\n",
    "\n",
    "However, recent research has seen some non-linear-layering which works out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Epoch [1/5], Iter [100/600] Loss: 0.1380\n",
      "Epoch [1/5], Iter [200/600] Loss: 0.1122\n",
      "Epoch [1/5], Iter [300/600] Loss: 0.0949\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0673\n",
      "Epoch [1/5], Iter [500/600] Loss: 0.1085\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.1074\n",
      "Epoch [2/5], Iter [100/600] Loss: 0.0440\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.0188\n",
      "Epoch [2/5], Iter [300/600] Loss: 0.1125\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0363\n",
      "Epoch [2/5], Iter [500/600] Loss: 0.0179\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.1303\n",
      "Epoch [3/5], Iter [100/600] Loss: 0.0377\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.0771\n",
      "Epoch [3/5], Iter [300/600] Loss: 0.0085\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.1492\n",
      "Epoch [3/5], Iter [500/600] Loss: 0.0219\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0538\n",
      "Epoch [4/5], Iter [100/600] Loss: 0.0148\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0082\n",
      "Epoch [4/5], Iter [300/600] Loss: 0.0095\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0205\n",
      "Epoch [4/5], Iter [500/600] Loss: 0.0083\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0295\n",
      "Epoch [5/5], Iter [100/600] Loss: 0.0091\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0071\n",
      "Epoch [5/5], Iter [300/600] Loss: 0.0148\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0138\n",
      "Epoch [5/5], Iter [500/600] Loss: 0.0359\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0310\n",
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/convolutional_neural_network/main.py\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# CNN Model (2 conv layer)\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        self.fc = nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        \n",
    "cnn = CNN()\n",
    "\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "cnn.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
