{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "06b9e325-df2f-4e9a-8a0f-5b0456c8c6c2"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "Until lecture 10.\n",
    "\n",
    "Source material: http://cs231n.stanford.edu/syllabus.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Activation Functions\n",
    "\n",
    "## Sigmoid\n",
    "\n",
    "$\\sigma(x) = 1/(1+e^{-x})$, output $\\in [0,1]$\n",
    "\n",
    "Problems:\n",
    "- saturation and killing of gradients: at the tails, gradients are almost 0\n",
    "- outputs not zero centered: at later layers, can lead to zigzagging\n",
    "\n",
    "## tanh\n",
    "\n",
    "$\\tanh(x) = 2\\sigma(2x) - 1$, output $\\in [-1,1]$\n",
    "\n",
    "At least this one is zero-centered, though first problem remains\n",
    "\n",
    "## Rectified Linear Unit (ReLU)\n",
    "\n",
    "$f(x) = \\max(0,x)$\n",
    "\n",
    "- + Greatly accelerates SGD learning rate (supposedly because of linear, non-saturating form)\n",
    "- + Simple matrix thresholding\n",
    "- - Can \"die\" if learning rate is too high\n",
    "\n",
    "## Leaky ReLU\n",
    "\n",
    "$f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x >= 0) (x)$ for some small $\\alpha$\n",
    "\n",
    "- Attempts to fix the death problem, opinions mixed on whether it does\n",
    "\n",
    "## Maxout\n",
    "\n",
    "$\\max(w_1^T x + b_1, w_2^T x + b_2)$\n",
    "\n",
    "- Generalized ReLU and leaky ReLU, but number of params is doubled\n",
    "\n",
    "## TLDR\n",
    "\n",
    "- Try ReLU, if that doesn't work well, try leaky ReLu, try Maxout, try tanh\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "## example from cs231n\n",
    "\n",
    "```\n",
    "x  y\n",
    " \\/\n",
    " *\n",
    " |    z\n",
    " a   /\n",
    "  \\ /\n",
    "   +\n",
    "   |\n",
    "   b\n",
    "   |\n",
    "   SUM\n",
    "   |\n",
    "   c\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-0.9019 -1.3741  0.5085 -0.0164\n",
      " 0.6451  0.8173  0.4622  1.7398\n",
      "-0.9370  0.3392  0.5834  0.1341\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n",
      "\n",
      " 0.0014  2.3757 -0.7652  0.2973\n",
      "-0.4328  0.5511 -1.1725 -0.3712\n",
      "-0.0399  0.1815 -0.9876  1.3819\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n",
      "\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      " 1  1  1  1\n",
      "[torch.FloatTensor of size 3x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N,D = 3,4\n",
    "x = Variable(torch.randn(N,D), # you can .cuda() this if compiled with GPU support\n",
    "             requires_grad=True)\n",
    "y = Variable(torch.randn(N,D),\n",
    "             requires_grad=True)\n",
    "z = Variable(torch.randn(N,D),\n",
    "             requires_grad=True)\n",
    "\n",
    "a = x*y\n",
    "b = a+z\n",
    "c = torch.sum(b)\n",
    "c.backward()\n",
    "\n",
    "print(x.grad.data)\n",
    "print(y.grad.data)\n",
    "print(z.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Abstraction Levels\n",
    "\n",
    "1. tensor - imperative ndarray (GPU)\n",
    "2. variable - computational graph node, stores data, gradient\n",
    "3. module - neural network layer, stores state or learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.8240e+00 -1.7631e+00  6.9515e-01  ...   6.3589e-01  1.0783e+00 -4.9382e-01\n",
      " 6.7872e-01 -3.3975e-02  2.3419e-01  ...   4.0380e-01  4.5888e-01  2.8980e-01\n",
      "-8.1475e-01  1.1521e+00 -1.2944e+00  ...  -3.9297e-01 -1.5169e-01 -5.4367e-03\n",
      "                ...                   ⋱                   ...                \n",
      " 5.5354e-01 -6.9006e-01  5.3435e-01  ...  -1.3042e+00 -3.2528e-01 -3.3605e-01\n",
      " 4.3530e-01  6.1585e-01 -1.0339e+00  ...  -2.9458e-01  4.5648e-01  5.1100e-01\n",
      "-4.3797e-01  1.0416e+00 -3.5381e-01  ...   1.0808e+00 -2.1105e+00  1.5029e+00\n",
      "[torch.FloatTensor of size 1000x100]\n",
      "\n",
      "Variable containing:\n",
      " 0.3657 -0.0616  0.4886  ...   0.0241  0.0367 -1.2944\n",
      " 0.0012 -2.3050 -0.5206  ...   0.1069  2.0296  0.3539\n",
      " 0.1414  1.2347  1.8882  ...   1.8816  0.5254  0.7998\n",
      "          ...             ⋱             ...          \n",
      "-0.6765 -1.1699  1.2648  ...  -0.3919 -0.6870  0.5645\n",
      " 0.2284 -0.8989  0.5521  ...  -0.5136  1.2102 -0.5416\n",
      " 0.1527 -0.0769 -0.2782  ...   0.4135 -0.0459  0.0758\n",
      "[torch.FloatTensor of size 100x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = Variable(torch.randn(N, D_in), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n",
    "w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "niters = 500\n",
    "for t in range(niters):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    w1.data -= learning_rate * w1.grad.data\n",
    "    w2.data -= learning_rate * w2.grad.data\n",
    "\n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n",
    "\n",
    "print(w1)\n",
    "print(w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "-7.1715e-03 -3.1891e-02  1.6240e-02  ...  -3.1562e-02  2.5376e-02  2.8484e-02\n",
      "-1.4594e-02 -2.0420e-02 -6.8582e-03  ...  -3.0192e-02  3.0163e-02 -8.2229e-03\n",
      "-2.2499e-02 -1.5067e-02  6.6661e-03  ...   5.0411e-03 -2.2978e-02 -9.7512e-03\n",
      "                ...                   ⋱                   ...                \n",
      "-3.0491e-03  2.1939e-02  1.8735e-02  ...   1.4093e-02  2.9189e-02  1.3363e-02\n",
      " 9.0013e-03  1.8239e-02 -8.3933e-03  ...  -1.3175e-03  2.8808e-02 -1.6543e-02\n",
      " 2.2596e-02  9.0188e-03 -3.6682e-02  ...   1.5825e-02 -8.7305e-03 -3.0213e-02\n",
      "[torch.FloatTensor of size 100x1000]\n",
      ", Parameter containing:\n",
      "1.00000e-02 *\n",
      "  2.6924\n",
      " -1.4046\n",
      " -2.0361\n",
      " -0.9474\n",
      "  1.4091\n",
      "  1.4870\n",
      " -0.7614\n",
      "  1.3763\n",
      "  1.1454\n",
      "  0.4435\n",
      " -0.4593\n",
      "  0.7179\n",
      " -1.3324\n",
      "  1.9364\n",
      "  2.8984\n",
      " -1.3482\n",
      "  2.4812\n",
      " -1.9819\n",
      "  0.5796\n",
      "  0.1031\n",
      "  2.1817\n",
      "  2.8111\n",
      "  4.3834\n",
      "  2.5566\n",
      " -0.5711\n",
      " -0.2461\n",
      "  1.7617\n",
      "  1.1214\n",
      "  3.6693\n",
      "  3.1884\n",
      " -1.0156\n",
      "  0.2164\n",
      "  2.9482\n",
      "  4.3420\n",
      "  2.2797\n",
      "  2.6839\n",
      "  2.8039\n",
      "  1.4191\n",
      "  0.7350\n",
      "  0.8771\n",
      " -1.9729\n",
      " -0.8221\n",
      "  1.7256\n",
      "  2.4716\n",
      " -1.1041\n",
      " -2.5721\n",
      "  0.8279\n",
      " -0.8662\n",
      "  2.6450\n",
      "  1.7318\n",
      "  2.6067\n",
      "  2.4402\n",
      "  2.7999\n",
      "  1.2257\n",
      "  4.4970\n",
      " -1.8470\n",
      "  2.1105\n",
      " -1.1127\n",
      " -2.9737\n",
      "  3.5814\n",
      "  1.1549\n",
      "  0.6483\n",
      " -1.9199\n",
      "  1.8547\n",
      "  2.1456\n",
      " -0.3622\n",
      "  3.5085\n",
      "  2.7242\n",
      " -2.6030\n",
      "  2.0942\n",
      "  1.9019\n",
      "  0.3936\n",
      "  1.2351\n",
      "  1.2512\n",
      "  1.5746\n",
      "  3.4611\n",
      " -1.8532\n",
      " -1.4011\n",
      " -1.4429\n",
      "  2.3673\n",
      " -0.6960\n",
      "  3.6573\n",
      "  1.8297\n",
      " -1.8636\n",
      "  2.8366\n",
      "  0.2536\n",
      " -2.1437\n",
      "  1.8018\n",
      "  1.3536\n",
      "  0.1222\n",
      " -1.3070\n",
      "  1.8707\n",
      " -0.8714\n",
      "  1.7067\n",
      "  1.0102\n",
      " -0.4575\n",
      "  4.0398\n",
      "  2.5374\n",
      " -1.4230\n",
      "  0.3783\n",
      "[torch.FloatTensor of size 100]\n",
      ", Parameter containing:\n",
      " 0.0155 -0.0971  0.0462  ...   0.0475  0.0474 -0.0296\n",
      "-0.0813 -0.0661 -0.0679  ...   0.0351 -0.0806  0.0371\n",
      " 0.0019 -0.0583  0.0049  ...  -0.0657  0.0947 -0.0062\n",
      "          ...             ⋱             ...          \n",
      "-0.0122  0.1151 -0.0721  ...  -0.0498  0.0962 -0.0728\n",
      "-0.0876  0.0312  0.0255  ...  -0.0401 -0.0621  0.0769\n",
      " 0.0781 -0.0034 -0.0668  ...   0.0033 -0.0892  0.0739\n",
      "[torch.FloatTensor of size 10x100]\n",
      ", Parameter containing:\n",
      "-0.0315\n",
      " 0.1129\n",
      " 0.0844\n",
      " 0.0241\n",
      " 0.0191\n",
      " 0.1003\n",
      "-0.0882\n",
      " 0.0658\n",
      "-0.0913\n",
      "-0.0007\n",
      "[torch.FloatTensor of size 10]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.nn import *\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "x = Variable(torch.randn(N, D_in), requires_grad=False)\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "model = Sequential(Linear(D_in, H), ReLU(), Linear(H, D_out))\n",
    "loss_fn = MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "niters = 500\n",
    "for t in range(niters):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    # model.zero_grad()\n",
    "    # loss.backward()\n",
    "    # \n",
    "    # for p in model.parameters():\n",
    "    #     p.data -= learning_rate * p.grad.data\n",
    "    # \n",
    "    # can be replaced with\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "$h_t = f_W (h_{t-1}, x_t)$\n",
    "\n",
    "- one-to-one (vanilla)\n",
    "- one-to-many (e.g. captioning)\n",
    "- many-to-one (e.g. sentiment)\n",
    "- many-to-many (e.g. translation)\n",
    "- many-to-many / stacked-one-to-one (e.g. video frame labeling)\n",
    "\n",
    "## Backpropagation through time\n",
    "\n",
    "Forward across entire sequence, backwards through entire sequence\n",
    "\n",
    "### Truncated version\n",
    "\n",
    "Run through chunks of sequence instead of whole sequence, so hidden states are carried around but not always propagated\n",
    "\n",
    "Problem:\n",
    "- if singular values > 1, gradients explode, so we clip\n",
    "- if singular values < 1, gradients vanish, we can use LSTM\n",
    "\n",
    "# Long Short Term Memory (LSTM)\n",
    "\n",
    "A vector of 4 gates, \n",
    "\n",
    "- f, forget gate: P(erase cell), e.g. sigmoid\n",
    "- i, input gate: P(write cell), e.g. sigmoid\n",
    "- g, thresholdy kind of gate: how much to write to cell, e.g. sigmoid\n",
    "- o, output gate: how much to reveal cell, e.g. tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
